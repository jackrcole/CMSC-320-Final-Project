# Introduction {#intro}

Welcome to our guide!

From big data to machine learning, as data science becomes an area of ever-increasing importance, it is essential for people to gain an understanding of the foundational theory and applications of data science. Whether it's for a job, freelancing, or personal fun, our aim is to teach you the basics so you can make beautiful data representations like this:

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

For this guide, we'll be using R, and our dataset will be Kaggle's global terrorism database, which can be accessed from here: https://www.kaggle.com/START-UMD/gtd?select=globalterrorismdb_0718dist.csv.


## Getting Started

First, put the tidyverse library into your R file.

```{r lib}
library(tidyverse)
```

This library comes with a variety of functions and operators to make data manipulation and tidying easier. For a comprehensive overview, see here: https://medium.com/@brianward1428/introduction-to-tidyverse-7b3dbf2337d5.

Next, download the csv file onto your machine. To access it, set the working directory to the directory the file is in, and read it into a data frame with a variable name ("df" in the example below):

```{r data}
setwd("~/Desktop")  ## Use the setwd() function and the path to change directories
df <- read.csv("globalterrorismdb_0718dist.csv")                 ## Use the read.csv() function with the file name to access csv file
head(df)                                                         ## Show top of df
```

As you can see, there is a lot of data to look through. But don't worry: we have tools to help us.

## Pipelines and Basic Functions

Some basic operations for R include:

-select() -> selects specific vertical columns in the dataset
-slice() -> selects specific horizontal rows in a dataset
-filter() -> selects specific rows in a dataset based on a specific value in a column
-mutate() -> add a new column to a dataset
-arrange() -> sort entities within a column
-summarize() -> creates a new table with summaries of the columns in the original dataset

One concept in data science is the use of a pipeline, which looks like "%>%" in R. This bit of "syntactic sugar" makes it easier to understand how the data is being manipulated. Here's some examples with the above functions:

```{r expl1}
# Data with attacks from September 1971
acts_sep71 <- df %>%            ## From df, have all occurances where iyear is 1971 and imonth is 9
  filter(iyear == 1971) %>%     ## This line filters out all years that are not 1971
  filter(imonth == 9)           ## This line filters out all months that are not 9 (Sept.)
head(acts_sep71)
```

```{r expl2}
# From September 1971 attacks, get coordinates of each attack
sep71_coord <- 
  acts_sep71 %>%                         ## From acts_sep71...
  select(eventid, country_txt, latitude, longitude)   ## Select the eventid, country_txt, latitude, and longitude columns
head(sep71_coord)
```

```{r expl 3}
# From the previous data frame, order the countries alphabetically and select the top 10
coords10 <- 
  sep71_coord %>%     ## From set71_coord...
  arrange(country_txt) %>%      ## arrange the country names in alphabetical order
  slice(1:10)                   ## select the ten terror acts at the top
coords10
```

```{r expl4}
# Summarize a few attributes of the September 1971 attacks
data_summary <- 
  acts_sep71 %>%                                            ## From dataset, summarize mean success rate...
  summarize(success_rate = mean(success), half_way_day = median(iday))    ## ...and day where data can be split in two
data_summary
```
For more reading on the basic operations (plus some extras), see here: https://www.hcbravo.org/IntroDataSci/bookdown-notes/principles-basic-operations.html.

## Data Cleaning

In data science, the way data is presented is important to the understanding of the data itself. Confusing variables or missing variables can negatively impact the understanding of what the data is trying to convey. Therefore, it is important to fix these errors when they arise.

Here are some examples:

```{r no_nas}
## Clean up NA's from coords10 dataset
clean_coords10 <- 
  sep71_coord %>%
  filter(!is.na(latitude)) %>%      ## Same as coords10, except all instances of NA in lat/long are filtered out
  arrange(country_txt) %>%
  slice(1:10)
clean_coords10
```

```{r to_true}
# Change success column values from 1/0 to TRUE/FALSE to ease readibility for success, multiple, and suicide
attrs <- c("multiple", "success", "suicide")            ## List of column names to change

for (name in attrs) {                                   ## In for loop, loop for every name              
  column <- acts_sep71[[name]]                          ## Select column with the name
  repl <- c()                                           ## Make a replacement vector "repl"
  for (x in column) {                                   ## In inner for loop, iterate over every element in selected column
    if (x == 1) {                                       ## Change 1 to TRUE
      repl <- c(repl, TRUE)                             ## Add to repl
    } else {                                            ## Change 0 to FALSE
      repl <- c(repl, FALSE)                            ## Add to repl
    }
  }
  if (length(repl) > length(acts_sep71$eventid)) {      ## If length of repl overshoots by 1 (can happen)
    repl <- repl %>% head(-1)                           ## Get rid of last element
  }
  acts_sep71[[name]] <- repl                            ## Replace old column with replacement
}

# Check
check <- 
  acts_sep71 %>% 
  select(multiple, success, suicide)
head(check)
```

There are also other ways to clean this dataset, such as getting rid of unused columns. For more data cleaning techniques, see here: https://www.hcbravo.org/IntroDataSci/bookdown-notes/tidying-data.html.

## Parsing and Management

Data parsing is turning input data from one type to another type. A simple example of this was seen in the last problem, in turing a numeric value to a more readable boolean value. Another example of this is reading HTML data from a website and turning it into something useful, such as a data frame. This can be seen in this example, where an online solar flare table has been converted into a data frame in R:

```{r swl}
library(rvest)                                                                            ## Library to use
url <- "https://www.spaceweatherlive.com/en/solar-activity/top-50-solar-flares"           ## Website URL
tables <- 
  url %>% 
  read_html() %>% 
  html_table(fill = TRUE)                                 ## Get array of tables
swl_data <- tables[[1]]                                                                   ## Get desired table
names(swl_data) <- c("rank", "flare_class", "date", "flare_region", "start_time",         ## Label columns accordingly
  "max_time", "end_time", "movie")
head(swl_data)
```

Using this code, we managed to parse the site data and turn it into a useful form we wanted.

Data management, on the other hand, is pretty self explanatory; it is basically the upkeep of a dataset, and ensuring that the data in question is correct, updated, and available for people to see. The website where we got our data, Kaggle, is a good example of this: the dataset was updated a few years ago with new data, people are able to provide feedback and report problems, and the visibility is set to public.

## Exploratory Data Analysis

If we want to discover meaningful conclusions from our dataset, we're going to need to do something called Exploratory Data Analysis, which is where we look at our data and see what we get.

Let's say we wanted to see what effect consumption of alcohol has on the suicide rate of a country. Let's get some data from the web and visualize it.

```{r eda}
library(tidyverse) # Import the necessary libraries
library(rvest)

alc_data <- read_html("https://en.wikipedia.org/wiki/List_of_countries_by_alcohol_consumption_per_capita") # Get HTML data from the source of info on alcohol

alc_df <- # Pull data from the proper HTML table
 alc_data %>%
 html_nodes("table") %>%
 .[[3]] %>%
 html_table()

alc_df <- # organize our data into a data frame and limit attributes
  alc_df %>%
  select("Country", "Total")

sui_df <- read_csv("~/Desktop/data.csv") # Import our suicide data

sui_df <- # Limit our suicide data to info on both genders
  filter(sui_df, Sex == "Both sexes") %>%
  select("Country", "2010")

df <-  # Join our data sets by the countries in both
  alc_df %>%
  inner_join(sui_df, by="Country")

df <-  # Make our column names easier to understand
  df %>%
  rename("Alcohol_Consumption" = "Total", "Suicide_Rate" ="2010")

df %>%
  ggplot(mapping = aes(x = Alcohol_Consumption, y = Suicide_Rate)) +
  geom_point()

```
Here we can see that while the correlation is somewhat weak, there is some link between how much alcohol people drink and the amount of people that commit suicide.

## Machine Learning

In this section we will run a simple linear regression to see what trends we can establish from our visualization.

A linear regression simply means that we are fitting our data to a line of the form $ax + b$, where $a$ and $b$ are real numbers. We want to do this in such a way that the distance between our actual data and the trend line established by our linear regression is as little as possible.

Let's see what our graph looks like with a linear regression line added.

```{r plot with linear}
df %>%
  ggplot(mapping = aes(x = Alcohol_Consumption, y = Suicide_Rate)) +
  geom_point() +
  geom_smooth(method=lm) 
```

Looks like there is a slight positive correlation. To see if this correlation is statistically significant, we'll do something called hypothesis testing.

## Hypothesis Testing

for experiments in statistics, we often employ what is known as hypothesis testing. Proper hypothesis testing ensures that we know whether or not our results matter.

Often, we assume the worst case, that our results don't matter. The statement that nothing worth noticing is happening is called the null hypothesis. In this case, the null hypothesis is that there is no correlation between a country's alcohol consumption and its suicide rate.

To determine if the slope we found through our linear regression means anything, we will look at our p-value, which is essentially the probability of getting results at least as extreme as what was actually observed.

If the slope that we found is statistically significant, we will have a p-value of greater than 0.05.

```{r hypothesis testing}
linear <- lm(Alcohol_Consumption ~ Suicide_Rate, data = df)
summary(linear)
```

Because our p-value is far less than 0.05, we can safely say that our positive correlation is statistically significant.

If you want to learn more about statistics and hypothesis testing, visit https://en.wikipedia.org/wiki/Statistical_hypothesis_testing.

## Message Curation

Download the dataset from https://www.kaggle.com/sudalairajkumar/undata-country-profiles

Here we take our global terrorism database from before and use it to make some additional observations.

```{r curation}
library(tidyverse)
library(lubridate)

csv_file1 <- "~/Desktop/globalterrorismdb_0718dist.csv"
csv_file2 <- "~/Desktop/country_profile_variables.csv"

tidy_terr <- read_csv(csv_file1)
names(tidy_terr)[names(tidy_terr)=="country"] <- "countryID"
names(tidy_terr)[names(tidy_terr)=="country_txt"] <- "country"
tidy_country <- read_csv(csv_file2)

tidy_merge <- merge(tidy_terr, tidy_country, by="country")


tidy_numcountry <- tidy_merge %>%
  select(country) %>%
  count(country)
tidy_numcountry
```

What we did was take data from another data set and merge the two by taking a column they had in common, in this case, the name of the country, and merging them. By using multiple data sets, we are able to use as much information as we want in our project, and by merging two datasets, we can relate them to each other.

What message can we can from this data? As we can see, the different countries vary wildly with regards to the amount of incidents of terrorism that take place in their country. Some countries, like Afghanistan, have thousands of incidents, while some countries, such as Andorra, have as little as one recorded incident of terrorism. This is valuable information for us to see which areas of the world have the biggest terrorism problem, but we should also keep in mind that our analysis is only as good as the data is comes from.