# Introduction {#intro}

Welcome to our guide!

As the field of data science grows in importance, from big data to machine learning, it becomes increasingly important to gain an insight on how the basics of data science theory and its applications work. Our goal is to introduce to you the foundations of data science so you are able to apply it in any job, freelancing projects, or personal use! By the end of this guide, we hope you are able plot beautiful data like this:

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

For this tutorial, we will be using R. Our dataset will be Kaggle's global terrorism dataset, developed by researchers at the University of Maryland. It can be accessed here: https://www.kaggle.com/START-UMD/gtd?select=globalterrorismdb_0718dist.csv.

## The Basics



Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

### Getting Started

First, put the tidyverse library into your R file.

```{r lib}
library(tidyverse)
```

This library comes with a variety of functions and operators to make data manipulation and tidying easier. For a comprehensive overview, see here: https://medium.com/@brianward1428/introduction-to-tidyverse-7b3dbf2337d5.

Next, download the csv file onto your machine. To access it, set the working directory to the directory the file is in, and read it into a data frame with a variable name ("df" in the example below):

```{r data}
setwd("C:/Users/Amit/Documents/Final Project/Global Terrorism")  ## Use the setwd() function and the path to change directories
df <- read.csv("globalterrorismdb_0718dist.csv")                 ## Use the read.csv() function with the file name to access csv file
head(df)                                                         ## Show top of df
```

As you can see, there is a lot of data to look through. But don't worry: we have tools to help us.

### Pipelines and Basic Functions

Some basic operations for R include:

-select() -> selects specific vertical columns in the dataset
-slice() -> selects specific horizontal rows in a dataset
-filter() -> selects specific rows in a dataset based on a specific value in a column
-mutate() -> add a new column to a dataset
-arrange() -> sort entities within a column
-summarize() -> creates a new table with summaries of the columns in the original dataset

One concept in data science is the use of a pipeline, which looks like "%>%" in R. This bit of "syntactic sugar" makes it easier to understand how the data is being manipulated. Here's some examples with the above functions:

```{r expl1}
# Data with attacks from September 1971
acts_sep71 <- df %>%            ## From df, have all occurances where iyear is 1971 and imonth is 9
  filter(iyear == 1971) %>%     ## This line filters out all years that are not 1971
  filter(imonth == 9)           ## This line filters out all months that are not 9 (Sept.)
head(acts_sep71)
```

```{r expl2}
# From September 1971 attacks, get coordinates of each attack
sep71_coord <- acts_sep71 %>%                         ## From acts_sep71...
  select(eventid, country_txt, latitude, longitude)   ## Select the eventid, country_txt, latitude, and longitude columns
head(sept71_coord)
```

```{r expl 3}
# From the previous data frame, order the countries alphabetically and select the top 10
coords10 <- sep71_coord %>%     ## From set71_coord...
  arrange(country_txt) %>%      ## arrange the country names in alphabetical order
  slice(1:10)                   ## select the ten terror acts at the top
coords10
```

```{r expl4}
# Summarize a few attributes of the September 1971 attacks
data_summary <- acts_sep71 %>%                                            ## From dataset, summarize mean success rate...
  summarize(success_rate = mean(success), half_way_day = median(iday))    ## ...and day where data can be split in two
data_summary
```
For more reading on the basic operations (plus some extras), see here: https://www.hcbravo.org/IntroDataSci/bookdown-notes/principles-basic-operations.html.

### Data Cleaning

In data science, the way data is presented is important to the understanding of the data itself. Confusing variables or missing variables can negatively impact the understanding of what the data is trying to convey. Therefore, it is important to fix these errors when they arise.

Here are some examples:

```{r no_nas}
## Clean up NA's from coords10 dataset
clean_coords10 <- sep71_coord %>%
  filter(!is.na(latitude)) %>%      ## Same as coords10, except all instances of NA in lat/long are filtered out
  arrange(country_txt) %>%
  slice(1:10)
clean_coords10
```

```{r to_true}
# Change success column values from 1/0 to TRUE/FALSE to ease readibility for success, multiple, and suicide
attrs <- c("multiple", "success", "suicide")            ## List of column names to change

for (name in attrs) {                                   ## In for loop, loop for every name              
  column <- acts_sep71[[name]]                          ## Select column with the name
  repl <- c()                                           ## Make a replacement vector "repl"
  for (x in column) {                                   ## In inner for loop, iterate over every element in selected column
    if (x == 1) {                                       ## Change 1 to TRUE
      repl <- c(repl, TRUE)                             ## Add to repl
    } else {                                            ## Change 0 to FALSE
      repl <- c(repl, FALSE)                            ## Add to repl
    }
  }
  if (length(repl) > length(acts_sep71$eventid)) {      ## If length of repl overshoots by 1 (can happen)
    repl <- repl %>% head(-1)                           ## Get rid of last element
  }
  acts_sep71[[name]] <- repl                            ## Replace old column with replacement
}

# Check
check <- acts_sep71 %>% 
  select(multiple, success, suicide)
head(check)
```

There are also other ways to clean this dataset, such as getting rid of unused columns. For more data cleaning techniques, see here: https://www.hcbravo.org/IntroDataSci/bookdown-notes/tidying-data.html.

### Parsing and Management

Data parsing is turning input data from one type to another type. A simple example of this was seen in the last problem, in turing a numeric value to a more readable boolean value. Another example of this is reading HTML data from a website and turning it into something useful, such as a data frame. This can be seen in this example, where an online solar flare table has been converted into a data frame in R:

```{r swl}
library(rvest)                                                                            ## Library to use
url <- "https://www.spaceweatherlive.com/en/solar-activity/top-50-solar-flares"           ## Website URL
tables <- url %>% read_html() %>% html_table(fill = TRUE)                                 ## Get array of tables
swl_data <- tables[[1]]                                                                   ## Get desired table
names(swl_data) <- c("rank", "flare_class", "date", "flare_region", "start_time",         ## Label columns accordingly
  "max_time", "end_time", "movie")
head(swl_data)
```

Using this code, we managed to parse the site data and turn it into a useful form we wanted.

Data management, on the other hand, is pretty self explanatory; it is basically the upkeep of a dataset, and ensuring that the data in question is correct, updated, and available for people to see. The website where we got our data, Kaggle, is a good example of this: the dataset was updated a few years ago with new data, people are able to provide feedback and report problems, and the visibility is set to public. 
